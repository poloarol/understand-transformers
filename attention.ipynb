{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b07b46",
   "metadata": {},
   "source": [
    "Here, I would be learning how to code Attention. \n",
    "\n",
    "It one of the components necessary for the success of Transformers, and more importantly LLMs and Multimodal models\n",
    "\n",
    "There are two types of `Attention`; self-attention and masked masked self-attention\n",
    "\n",
    "`self-attention` understands the meaning of words within their respective context by studying words before and after it, inclusding itself. It is common in `encoder-only` models such as `BERT` models\n",
    "\n",
    "`masked self-attention` understands the meaning of words within their respective context by studying words before it and itself. It is common in both `decoder-only` models such as OpenAI `GPT-series` models\n",
    "\n",
    "`encoder-decoder attention` uses both `self-attention` in the encoder, and `masked self-attention` in the decoder. It is common in `multimodal models` such as `T5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d110b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive\\Desktop\\GitHub\\understand-transformers\\env\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89643bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1):\n",
    "        ## d_model = the number of embedding values per token.\n",
    "        ##           Because we want to be able to do the math by hand, we've\n",
    "        ##           the default value for d_model=2.\n",
    "        ##           However, in \"Attention Is All You Need\" d_model=512\n",
    "        ##\n",
    "        ## row_dim, col_dim = the indices we should use to access rows or columns\n",
    "\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ## Initialize the Weights (W) that we'll use to create the\n",
    "        ## query (q), key (k) and value (v) for each token\n",
    "        ## NOTE: A lot of implementations include bias terms when\n",
    "        ##       creating the the queries, keys, and values, but\n",
    "        ##       the original manuscript that described Attention,\n",
    "        ##       \"Attention Is All You Need\" did not, so we won't either\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "        \n",
    "    def forward(self, token_encodings, mask=None):\n",
    "        ## Create the query, key and values using the encoding numbers\n",
    "        ## associated with each token (token encodings)\n",
    "        q = self.W_q(token_encodings)\n",
    "        k = self.W_k(token_encodings)\n",
    "        v = self.W_v(token_encodings)\n",
    "\n",
    "        ## Compute similarities scores: (q * k^T)\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        ## Scale the similarities by dividing by sqrt(k.col_dim)\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ## Here we are masking out things we don't want to pay attention to\n",
    "            ##\n",
    "            ## We replace values we wanted masked out\n",
    "            ## with a very small negative number so that the SoftMax() function\n",
    "            ## will give all masked elements an output value (or \"probability\") of 0.\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e20) # I've also seen -1e20 and -9e15 used in masking\n",
    "\n",
    "        ## Apply softmax to determine what percent of each tokens' value to\n",
    "        ## use in the final attention values.\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        ## Scale the values by their associated percentages and add them up.\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d085ffd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2557b43cc10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a matrix of token encodings...\n",
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])\n",
    "\n",
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37638dce",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af3788ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7081, -0.8268],\n",
       "        [-0.7417, -0.9193],\n",
       "        [-0.7190, -0.8447]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a basic self-attention ojbect\n",
    "selfAttention = Attention(d_model=2,\n",
    "                               row_dim=0,\n",
    "                               col_dim=1)\n",
    "\n",
    "## calculate basic attention for the token encodings\n",
    "selfAttention(encodings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612b6ef",
   "metadata": {},
   "source": [
    "## Masked Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4dda54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a masked self-attention object\n",
    "maskedSelfAttention = Attention(d_model=2,\n",
    "                               row_dim=0,\n",
    "                               col_dim=1)\n",
    "\n",
    "## create the mask so that we don't use\n",
    "## tokens that come after a token of interest\n",
    "mask = torch.tril(torch.ones(3, 3))\n",
    "mask = mask == 0\n",
    "mask # print out the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d118b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.0062,  0.6072],\n",
       "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate masked self-attention\n",
    "maskedSelfAttention(encodings_matrix, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
